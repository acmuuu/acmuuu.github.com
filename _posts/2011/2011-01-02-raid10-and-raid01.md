---
layout: post
category : Linux
tags : [Raid]
title: Raid01和Raid10
---
{% include JB/setup %}
RAID 0又称为Stripe或Striping，它代表了所有RAID级别中最高的存储性能。RAID 0提高存储性能的原理是把连续的数据分散到多个磁盘上存取，这样，系统有数据请求就可以被多个磁盘并行的执行，每个磁盘执行属于它自己的那部分数据请求。这种数据上的并行操作可以充分利用总线的带宽，显著提高磁盘整体存取性能

![Alt text](/assets/images/2011/01/raid0.png)

RAID 1又称为Mirror或Mirroring，它的宗旨是最大限度的保证用户数据的可用性和可修复性。 RAID 1的操作方式是把用户写入硬盘的数据百分之百地自动复制到另外一个硬盘上。由于对存储的数据进行百分之百的备份，在所有RAID级别中，RAID 1提供最高的数据安全保障。同样，由于数据的百分之百备份，备份数据占了总存储空间的一半，因而，Mirror的磁盘空间利用率低，存储成本高。Mirror虽不能提高存储性能，但由于其具有的高数据安全性，使其尤其适用于存放重要数据，如服务器和数据库存储等领域。

![Alt text](/assets/images/2011/01/raid1.png)

RAID 1+0是先镜射再分区数据。是将所有硬盘分为两组，视为是RAID 0的最低组合，然后将这两组各自视为RAID 1运作。RAID 1+0有着不错的读取速度，而且拥有比RAID 0更高的数据保护性。

![Alt text](/assets/images/2011/01/raid10.gif)

RAID 0+1则是跟RAID 1+0的程序相反，是先分区再将数据镜射到两组硬盘。它将所有的硬盘分为两组，变成RAID 1的最低组合，而将两组硬盘各自视为RAID 0运作。RAID 0+1比起RAID 1+0有着更快的读写速度，不过也多了一些会让整个硬盘组停止运转的机率；因为只要同一组的硬盘全部损毁，RAID 0+1就会停止运作，而RAID 1+0则可以在牺牲RAID 0的优势下正常运作。

![Alt text](/assets/images/2011/01/raid01.gif)

RAID 10/01巧妙的利用了RAID 0的速度以及RAID 1的保护两种特性，不过它的缺点是需要的硬盘数较多，因为至少必须拥有四个以上的偶数硬盘才能使用。

 

**吞吐量与IOPS转自piner的博客 [http://www.ixdba.com/](http://www.ixdba.com/)**

阵列的瓶颈主要体现在2个方面，吞吐量与IOPS。

**1、吞吐量**

吞吐量主要取决于阵列的构架，光纤通道的大小（现在阵列一般都是光纤阵列，至于SCSI这样的SSA阵列，我们不讨论）以及硬盘的个数。阵列的构架与每个阵列不同而不同，他们也都存在内部带宽（类似于pc的系统总线），不过一般情况下，内部带宽都设计的很充足，不是瓶颈的所在。

光纤通道的影响还是比较大的，如数据仓库环境中，对数据的流量要求很大，而一块2Gb的光纤卡，所能支撑的最大流量应当是2Gb/8(小B)=250MB/s(大B)的实际流量，当4块光纤卡才能达到1GB/s的实际流量，所以数据仓库环境可以考虑换4Gb的光纤卡。

最后说一下硬盘的限制，这里是最重要的，当前面的瓶颈不再存在的时候，就要看硬盘的个数了，我下面列一下不同的硬盘所能支撑的流量大小：

	10K rpm     15K rpm       ATA
	———         ———           ———
	10M/s       13M/s         8M/s
		  
那么，假定一个阵列有120块15K rpm的光纤硬盘，那么硬盘上最大的可以支撑的流量为120*13=1560MB/s，如果是2Gb的光纤卡，可能需要6块才能够，而4Gb的光纤卡，3-4块就够了。

**2、IOPS**

决定IOPS的主要取决与阵列的算法，cache命中率，以及磁盘个数。阵列的算法因为不同的阵列不同而不同，如我们最近遇到在hds usp上面，可能因为ldev(lun)存在队列或者资源限制，而单个ldev的iops就上不去，所以，在使用这个存储之前，有必要了解这个存储的一些算法规则与限制。

cache的命中率取决于数据的分布，cache size的大小，数据访问的规则，以及cache的算法，如果完整的讨论下来，这里将变得很复杂，可以有一天好讨论了。我这里只强调一个cache的命中率，如果一个阵列，读cache的命中率越高越好，一般表示它可以支持更多的IOPS，为什么这么说呢？这个就与我们下面要讨论的硬盘IOPS有关系了。

硬盘的限制，每个物理硬盘能处理的IOPS是有限制的，如

	10K rpm     15K rpm       ATA
	———         ———           ———
	100         150           50
	  
同样，如果一个阵列有120块15K rpm的光纤硬盘，那么，它能撑的最大IOPS为120*150=18000，这个为硬件限制的理论值，如果超过这个值，硬盘的响应可能会变的非常缓慢而不能正常提供业务。

另外，我们上一篇也讨论了，在raid5与raid10上，读iops没有差别，但是，相同的业务写iops，最终落在磁盘上的iops是有差别的，而我们评估的却正是磁盘的IOPS，如果达到了磁盘的限制，性能肯定是上不去了。

那我们假定一个case，业务的iops是10000，读cache命中率是30%，读iops为60%，写iops为40%，磁盘个数为120，那么分别计算在raid5与raid10的情况下，每个磁盘的iops为多少。

**raid5：**

单块盘的

	iops = (10000*(1-0.3)*0.6 + 4 * (10000*0.4))/120
	= (4200 + 16000)/120
	= 168

这里的10000*(1-0.3)*0.6表示是读的iops，比例是0.6，除掉cache命中，实际只有4200个iops而4 * (10000*0.4) 表示写的iops，因为每一个写，在raid5中，实际发生了4个io，所以写的iops为16000个

为了考虑raid5在写操作的时候，那2个读操作也可能发生命中，所以更精确的计算为：

单块盘的

	iops = (10000*(1-0.3)*0.6 + 2 * (10000*0.4)*(1-0.3) + 2 * (10000*0.4))/120
	= (4200 + 5600 + 8000)/120
	= 148

计算出来单个盘的iops为148个，基本达到磁盘极限

**raid10：**

单块盘的

	iops = (10000*(1-0.3)*0.6 + 2 * (10000*0.4))/120
	= (4200 + 8000)/120
	= 102

可以看到，因为raid10对于一个写操作，只发生2次io，所以，同样的压力，同样的磁盘，每个盘的iops只有102个，还远远低于磁盘的极限iops。

在一个实际的case中，一个恢复压力很大的standby（这里主要是写，而且是小io的写），采用了raid5的方案，发现性能很差，通过分析，每个磁盘的iops在高峰时期，快达到200了，导致响应速度巨慢无比。后来改造成raid10，就避免了这个性能问题，每个磁盘的iops降到100左右。